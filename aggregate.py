"""
Aggregator — merges all raw scraped data, runs VADER sentiment analysis,
and writes a unified mentions.json file for the dashboard.
Output: data/mentions.json
"""
import json
import os
from datetime import datetime

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

DATA_DIR = "/data/data"
OUTPUT_FILE = os.path.join(DATA_DIR, "mentions.json")

RAW_FILES = [
    "reddit_raw.json",
    "twitter_raw.json",
    "playstore_raw.json",
    "appstore_raw.json",
]

analyzer = SentimentIntensityAnalyzer()


def get_sentiment(text):
    if not text or not text.strip():
        return {"label": "neutral", "compound": 0.0, "pos": 0.0, "neu": 1.0, "neg": 0.0}
    scores = analyzer.polarity_scores(text)
    compound = scores["compound"]
    if compound >= 0.05:
        label = "positive"
    elif compound <= -0.05:
        label = "negative"
    else:
        label = "neutral"
    return {
        "label": label,
        "compound": round(compound, 4),
        "pos": round(scores["pos"], 4),
        "neu": round(scores["neu"], 4),
        "neg": round(scores["neg"], 4),
    }


def get_text_for_sentiment(record):
    """Pick the best field to run sentiment on."""
    platform = record.get("platform", "")
    if platform == "reddit":
        return f"{record.get('title', '')} {record.get('text', '')}".strip()
    elif platform == "playstore":
        return record.get("text", "")
    else:
        return record.get("text", "")


def aggregate():
    os.makedirs(DATA_DIR, exist_ok=True)
    all_records = []
    seen_ids = set()

    for filename in RAW_FILES:
        path = os.path.join(DATA_DIR, filename)
        if not os.path.exists(path):
            print(f"[Aggregate] Skipping missing file: {filename}")
            continue

        with open(path, "r", encoding="utf-8") as f:
            try:
                records = json.load(f)
            except json.JSONDecodeError:
                print(f"[Aggregate] Failed to parse {filename}")
                continue

        for record in records:
            uid = f"{record.get('platform', '')}_{record.get('id', '')}"
            if uid in seen_ids:
                continue
            seen_ids.add(uid)

            text_for_sentiment = get_text_for_sentiment(record)
            sentiment = get_sentiment(text_for_sentiment)
            record["sentiment"] = sentiment
            all_records.append(record)

    # Sort by created_at descending
    def sort_key(r):
        return r.get("created_at") or r.get("scraped_at") or ""

    all_records.sort(key=sort_key, reverse=True)

    # Calculate stats
    platform_counts = {}
    sentiment_counts = {"positive": 0, "neutral": 0, "negative": 0}

    for r in all_records:
        p = r.get("platform", "unknown")
        platform_counts[p] = platform_counts.get(p, 0) + 1
        s = r.get("sentiment", {}).get("label", "neutral")
        if s in sentiment_counts:
            sentiment_counts[s] += 1

    meta = {
        "last_updated": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%S") + "Z",
        "total": len(all_records),
        "by_platform": platform_counts,
        "by_sentiment": sentiment_counts,
    }

    output = {"meta": meta, "records": all_records}

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    # Also write as a JS file so the dashboard works via file:// protocol
    js_file = os.path.join(DATA_DIR, "data.js")
    with open(js_file, "w", encoding="utf-8") as f:
        f.write("// Auto-generated by aggregate.py — do not edit\n")
        f.write("window.MATIKS_DATA = ")
        json.dump(output, f, ensure_ascii=False)
        f.write(";\n")

    print(f"[Aggregate] Done. Total records: {len(all_records)}")
    print(f"  Platforms: {meta['by_platform']}")
    print(f"  Sentiment: {meta['by_sentiment']}")


if __name__ == "__main__":
    aggregate()
